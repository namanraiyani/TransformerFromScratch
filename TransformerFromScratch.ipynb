{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNisaqJDbinvk2/3TsVmdI4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e12d931245824edebbf15adddef4f372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c8a5f349e674b7192838afd07dd6eea",
              "IPY_MODEL_eded18c177c243ba8e410f36049a4fae",
              "IPY_MODEL_61acc558793f40b9990e0520e6c769ec"
            ],
            "layout": "IPY_MODEL_559292effa234a3680003cc457e56d7c"
          }
        },
        "4c8a5f349e674b7192838afd07dd6eea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ead4bef28a034957b0f9d8f13a8d470e",
            "placeholder": "​",
            "style": "IPY_MODEL_3de1435a9ab846e089bdc9793bda6b57",
            "value": "README.md: "
          }
        },
        "eded18c177c243ba8e410f36049a4fae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cacd8c4dea1c45839e69421f26de6c58",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_169eee2379f348ae8ba91a5051d4c0a4",
            "value": 1
          }
        },
        "61acc558793f40b9990e0520e6c769ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bafb7d523ca94395bdde6ad459ea629a",
            "placeholder": "​",
            "style": "IPY_MODEL_6c8cdffd4c574023b237d6d38d03d95b",
            "value": " 28.1k/? [00:00&lt;00:00, 869kB/s]"
          }
        },
        "559292effa234a3680003cc457e56d7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ead4bef28a034957b0f9d8f13a8d470e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3de1435a9ab846e089bdc9793bda6b57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cacd8c4dea1c45839e69421f26de6c58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "169eee2379f348ae8ba91a5051d4c0a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bafb7d523ca94395bdde6ad459ea629a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c8cdffd4c574023b237d6d38d03d95b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namanraiyani/TransformerFromScratch/blob/main/TransformerFromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U datasets huggingface_hub fsspec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cE7oi6aEqciO",
        "outputId": "0680b4c9-d841-4f30-a928-db6b2250ef93"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.33.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.2)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.33.1-py3-none-any.whl (515 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.4/515.4 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, huggingface_hub, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.33.0\n",
            "    Uninstalling huggingface-hub-0.33.0:\n",
            "      Successfully uninstalled huggingface-hub-0.33.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0 huggingface_hub-0.33.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import get_dataset_config_names\n",
        "get_dataset_config_names('opus_books')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e12d931245824edebbf15adddef4f372",
            "4c8a5f349e674b7192838afd07dd6eea",
            "eded18c177c243ba8e410f36049a4fae",
            "61acc558793f40b9990e0520e6c769ec",
            "559292effa234a3680003cc457e56d7c",
            "ead4bef28a034957b0f9d8f13a8d470e",
            "3de1435a9ab846e089bdc9793bda6b57",
            "cacd8c4dea1c45839e69421f26de6c58",
            "169eee2379f348ae8ba91a5051d4c0a4",
            "bafb7d523ca94395bdde6ad459ea629a",
            "6c8cdffd4c574023b237d6d38d03d95b"
          ]
        },
        "collapsed": true,
        "id": "8xdWSvk8sVca",
        "outputId": "fe00a5aa-decb-4b68-ba63-fc2773895ec4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e12d931245824edebbf15adddef4f372"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ca-de',\n",
              " 'ca-en',\n",
              " 'ca-hu',\n",
              " 'ca-nl',\n",
              " 'de-en',\n",
              " 'de-eo',\n",
              " 'de-es',\n",
              " 'de-fr',\n",
              " 'de-hu',\n",
              " 'de-it',\n",
              " 'de-nl',\n",
              " 'de-pt',\n",
              " 'de-ru',\n",
              " 'el-en',\n",
              " 'el-es',\n",
              " 'el-fr',\n",
              " 'el-hu',\n",
              " 'en-eo',\n",
              " 'en-es',\n",
              " 'en-fi',\n",
              " 'en-fr',\n",
              " 'en-hu',\n",
              " 'en-it',\n",
              " 'en-nl',\n",
              " 'en-no',\n",
              " 'en-pl',\n",
              " 'en-pt',\n",
              " 'en-ru',\n",
              " 'en-sv',\n",
              " 'eo-es',\n",
              " 'eo-fr',\n",
              " 'eo-hu',\n",
              " 'eo-it',\n",
              " 'eo-pt',\n",
              " 'es-fi',\n",
              " 'es-fr',\n",
              " 'es-hu',\n",
              " 'es-it',\n",
              " 'es-nl',\n",
              " 'es-no',\n",
              " 'es-pt',\n",
              " 'es-ru',\n",
              " 'fi-fr',\n",
              " 'fi-hu',\n",
              " 'fi-no',\n",
              " 'fi-pl',\n",
              " 'fr-hu',\n",
              " 'fr-it',\n",
              " 'fr-nl',\n",
              " 'fr-no',\n",
              " 'fr-pl',\n",
              " 'fr-pt',\n",
              " 'fr-ru',\n",
              " 'fr-sv',\n",
              " 'hu-it',\n",
              " 'hu-nl',\n",
              " 'hu-no',\n",
              " 'hu-pl',\n",
              " 'hu-pt',\n",
              " 'hu-ru',\n",
              " 'it-nl',\n",
              " 'it-pt',\n",
              " 'it-ru',\n",
              " 'it-sv']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import math\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "class InputEmbeddings(nn.Module):\n",
        "  def __init__(self, embedding_dim, vocab_size):   # embedding_dim is d_model\n",
        "    super().__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.embedding(x) * math.sqrt(self.embedding_dim) # scale embeddings to match positional encoding scale\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, embedding_dim, sequence_len, dropout):\n",
        "    super().__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.sequence_len = sequence_len\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    PE = torch.zeros(sequence_len, embedding_dim)\n",
        "    position = torch.arange(0, sequence_len, dtype = torch.float).unsqueeze(1)\n",
        "\n",
        "    denominator_term = torch.exp(torch.arange(0, embedding_dim, step = 2).float() * (-math.log(10000.0) / embedding_dim))\n",
        "\n",
        "    PE[:, 0::2] = torch.sin(position * denominator_term)\n",
        "    PE[:, 1::2] = torch.cos(position * denominator_term)\n",
        "    PE = PE.unsqueeze(0)\n",
        "\n",
        "    self.register_buffer('PE', PE)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + (self.PE[:, :x.shape[1], :]).requires_grad_(False)\n",
        "    return self.dropout(x)\n",
        "\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "  def __init__(self, embedding_dim, h, dropout):\n",
        "    super().__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.h = h\n",
        "\n",
        "    assert embedding_dim % h == 0, \"embedding_dim is not divisible by h\"\n",
        "\n",
        "    self.d_k = embedding_dim // h\n",
        "    self.w_q = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.w_k = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.w_v = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.w_o = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query, key, value, mask, dropout):\n",
        "    d_k = query.shape[-1]\n",
        "    attention_score = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "      attention_score.masked_fill_(mask == 0, -1e9)\n",
        "    attention_score = attention_score.softmax(dim = -1)\n",
        "    if dropout is not None:\n",
        "      attention_score = dropout(attention_score)\n",
        "    return (attention_score @ value), attention_score\n",
        "\n",
        "  def forward(self, q, k, v, mask):\n",
        "    query = self.w_q(q)\n",
        "    key = self.w_k(k)\n",
        "    value = self.w_v(v)\n",
        "\n",
        "    query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "    key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "    value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "\n",
        "    x, self.attention_score = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "    x = x.transpose(2,1).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "    return self.w_o(x)\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self, eps = 10**-6):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "\n",
        "    self.gamma = nn.Parameter(torch.ones(1))\n",
        "    self.beta = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    std = x.std(dim=-1, keepdim=True)\n",
        "    return (self.gamma * ((x - mean) / (std + self.eps))) + self.beta\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout):\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self, dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, self_attention_block, feed_forward_block, dropout):\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "  def forward(self, x, src_mask):\n",
        "      x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "      x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "      return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, layers):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, self_attention_block, cross_attention_block, feed_forward_block, dropout):\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.cross_attention_block = cross_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
        "\n",
        "  def forward(self, x, encoder_output, src_mask, target_mask):\n",
        "    x = self.residual_connections[0](x, lambda x:self.self_attention_block(x,x,x,target_mask))\n",
        "    x = self.residual_connections[1](x, lambda x:self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "    x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "    return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, layers):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self, x, encoder_output, src_mask, target_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, encoder_output, src_mask, target_mask)\n",
        "    return self.norm(x)\n",
        "\n",
        "class ProjectionLayer(nn.Module):\n",
        "  def __init__(self, embedding_dim, vocab_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.projection = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return torch.log_softmax(self.projection(x), dim=-1)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder, decoder, src_embedding, target_embedding, src_positional_encoding, target_positional_encoding, projection_layer):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embedding = src_embedding\n",
        "    self.target_embedding = target_embedding\n",
        "    self.src_positional_encoding = src_positional_encoding\n",
        "    self.target_positional_encoding = target_positional_encoding\n",
        "    self.projection_layer = projection_layer\n",
        "\n",
        "  def encode(self, src, src_mask):\n",
        "    src = self.src_embedding(src)\n",
        "    src = self.src_positional_encoding(src)\n",
        "    return self.encoder(src, src_mask)\n",
        "\n",
        "  def decode(self, encoder_output, src_mask, target, target_mask):\n",
        "    target = self.target_embedding(target)\n",
        "    target = self.target_positional_encoding(target)\n",
        "    return self.decoder(target, encoder_output, src_mask, target_mask)\n",
        "\n",
        "  def project(self, x):\n",
        "    return self.projection_layer(x)\n",
        "def build_transformer(src_vocab_size, target_vocab_size, src_seq_len, target_seq_len, embedding_dim = 512, N = 6, h = 8, dropout = 0.1, d_ff = 2048):\n",
        "  src_embedding = InputEmbeddings(embedding_dim, src_vocab_size)\n",
        "  target_embedding = InputEmbeddings(embedding_dim, target_vocab_size)\n",
        "  src_positional_encoding = PositionalEncoding(embedding_dim, src_seq_len, dropout)\n",
        "  target_positional_encoding = PositionalEncoding(embedding_dim, target_seq_len, dropout)\n",
        "\n",
        "  encoder_blocks = []\n",
        "  for _ in range(N):\n",
        "    encoder_self_attention_block = MultiHeadAttentionBlock(embedding_dim, h, dropout)\n",
        "    feed_forward_block = FeedForwardBlock(embedding_dim, d_ff, dropout)\n",
        "    encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
        "    encoder_blocks.append(encoder_block)\n",
        "\n",
        "  decoder_blocks = []\n",
        "  for _ in range(N):\n",
        "    decoder_self_attention_block = MultiHeadAttentionBlock(embedding_dim, h, dropout)\n",
        "    decoder_cross_attention_block = MultiHeadAttentionBlock(embedding_dim, h, dropout)\n",
        "    feed_forward_block = FeedForwardBlock(embedding_dim, d_ff, dropout)\n",
        "    decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "    decoder_blocks.append(decoder_block)\n",
        "\n",
        "  encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
        "  decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
        "\n",
        "  projection_layer = ProjectionLayer(embedding_dim, target_vocab_size)\n",
        "  transformer = Transformer(encoder, decoder, src_embedding, target_embedding, src_positional_encoding, target_positional_encoding, projection_layer)\n",
        "\n",
        "  for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "\n",
        "  return transformer\n",
        "\n",
        "def get_all_sentences(ds, lang):\n",
        "    for pair in ds:\n",
        "        yield pair['translation'][lang]\n",
        "\n",
        "class BilingualDataset(Dataset):\n",
        "\n",
        "    # This takes in the dataset contaning sentence pairs, the tokenizers for target and source languages, and the strings of source and target languages\n",
        "    # 'seq_len' defines the sequence length for both languages\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "\n",
        "        # Defining special tokens by using the target language tokenizer\n",
        "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "\n",
        "\n",
        "    # Total number of instances in the dataset (some pairs are larger than others)\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    # Using the index to retrive source and target texts\n",
        "    def __getitem__(self, index) :\n",
        "        src_target_pair = self.ds[index]\n",
        "        src_text = src_target_pair['translation'][self.src_lang]\n",
        "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "        # Tokenizing source and target texts\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "        # Computing how many padding tokens need to be added to the tokenized texts\n",
        "        # Source tokens\n",
        "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 # Subtracting the two '[EOS]' and '[SOS]' special tokens\n",
        "        # Target tokens\n",
        "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # Subtracting the '[SOS]' special token\n",
        "\n",
        "        # If the texts exceed the 'seq_len' allowed, it will raise an error. This means that one of the sentences in the pair is too long to be processed\n",
        "        # given the current sequence length limit (this will be defined in the config dictionary below)\n",
        "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "            raise ValueError('Sentence is too long')\n",
        "\n",
        "        # Building the encoder input tensor by combining several elements\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "            self.sos_token, # inserting the '[SOS]' token\n",
        "            torch.tensor(enc_input_tokens, dtype = torch.int64), # Inserting the tokenized source text\n",
        "            self.eos_token, # Inserting the '[EOS]' token\n",
        "            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Building the decoder input tensor by combining several elements\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token, # inserting the '[SOS]' token\n",
        "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
        "            ]\n",
        "\n",
        "        )\n",
        "\n",
        "        # Creating a label tensor, the expected output for training the model\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
        "                self.eos_token, # Inserting the '[EOS]' token\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Adding padding tokens\n",
        "\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Ensuring that the length of each tensor above is equal to the defined 'seq_len'\n",
        "        assert encoder_input.size(0) == self.seq_len\n",
        "        assert decoder_input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "        return {\n",
        "            'encoder_input': encoder_input,\n",
        "            'decoder_input': decoder_input,\n",
        "            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
        "            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n",
        "            'label': label,\n",
        "            'src_text': src_text,\n",
        "            'tgt_text': tgt_text\n",
        "        }\n",
        "\n",
        "def get_ds(config):\n",
        "\n",
        "    # Loading the train portion of the OpusBooks dataset.\n",
        "    # The Language pairs will be defined in the 'config' dictionary we will build later\n",
        "    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split = 'train')\n",
        "\n",
        "    # Building or loading tokenizer for both the source and target languages\n",
        "    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "    # Splitting the dataset for training and validation\n",
        "    train_ds_size = int(0.9 * len(ds_raw)) # 90% for training\n",
        "    val_ds_size = len(ds_raw) - train_ds_size # 10% for validation\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size]) # Randomly splitting the dataset\n",
        "\n",
        "    # Processing data with the BilingualDataset class, which we will define below\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "    # Iterating over the entire dataset and printing the maximum length found in the sentences of both the source and target languages\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "    for pair in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(pair['translation'][config['lang_src']]).ids\n",
        "        tgt_ids = tokenizer_src.encode(pair['translation'][config['lang_tgt']]).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {max_len_src}')\n",
        "    print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "    # Creating dataloaders for the training and validadion sets\n",
        "    # Dataloaders are used to iterate over the dataset in batches during training and validation\n",
        "    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True) # Batch size will be defined in the config dictionary\n",
        "    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt # Returning the DataLoader objects and tokenizers\n",
        "\n",
        "def casual_mask(size):\n",
        "        # Creating a square matrix of dimensions 'size x size' filled with ones\n",
        "        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n",
        "        return mask == 0\n",
        "\n",
        "def build_tokenizer(config, ds, lang):\n",
        "\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]'))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\",\n",
        "                                                     \"[SOS]\", \"[EOS]\"], min_frequency = 2)\n",
        "\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer = trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    return tokenizer\n",
        "\n",
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "    # Retrieving the indices from the start and end of sequences of the target tokens\n",
        "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
        "\n",
        "    # Computing the output of the encoder for the source sequence\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    # Initializing the decoder input with the Start of Sentence token\n",
        "    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
        "\n",
        "    # Looping until the 'max_len', maximum length, is reached\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "\n",
        "        # Building a mask for the decoder input\n",
        "        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "        # Calculating the output of the decoder\n",
        "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "        # Applying the projection layer to get the probabilities for the next token\n",
        "        prob = model.project(out[:, -1])\n",
        "\n",
        "        # Selecting token with the highest probability\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
        "\n",
        "        # If the next token is an End of Sentence token, we finish the loop\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0) # Sequence of tokens generated by the decoder\n",
        "\n",
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
        "    model.eval() # Setting model to evaluation mode\n",
        "    count = 0 # Initializing counter to keep track of how many examples have been processed\n",
        "\n",
        "    console_width = 80 # Fixed witdh for printed messages\n",
        "\n",
        "    # Creating evaluation loop\n",
        "    with torch.no_grad(): # Ensuring that no gradients are computed during this process\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)\n",
        "\n",
        "            # Ensuring that the batch_size of the validation set is 1\n",
        "            assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n",
        "\n",
        "            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n",
        "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "            # Retrieving source and target texts from the batch\n",
        "            source_text = batch['src_text'][0]\n",
        "            target_text = batch['tgt_text'][0] # True translation\n",
        "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n",
        "\n",
        "            # Printing results\n",
        "            print_msg('-'*console_width)\n",
        "            print_msg(f'SOURCE: {source_text}')\n",
        "            print_msg(f'TARGET: {target_text}')\n",
        "            print_msg(f'PREDICTED: {model_out_text}')\n",
        "\n",
        "            # After two examples, we break the loop\n",
        "            if count == num_examples:\n",
        "                break\n",
        "\n",
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "\n",
        "    # Loading model using the 'build_transformer' function.\n",
        "    # We will use the lengths of the source language and target language vocabularies, the 'seq_len', and the dimensionality of the embeddings\n",
        "    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n",
        "    return model\n",
        "\n",
        "def get_config():\n",
        "    return{\n",
        "        'batch_size': 8,\n",
        "        'num_epochs': 20,\n",
        "        'lr': 10**-4,\n",
        "        'seq_len': 350,\n",
        "        'd_model': 512, # Dimensions of the embeddings in the Transformer. 512 like in the \"Attention Is All You Need\" paper.\n",
        "        'lang_src': 'en',\n",
        "        'lang_tgt': 'it',\n",
        "        'model_folder': 'weights',\n",
        "        'model_basename': 'tmodel_',\n",
        "        'preload': None,\n",
        "        'tokenizer_file': 'tokenizer_{0}.json',\n",
        "        'experiment_name': 'runs/tmodel'\n",
        "    }\n",
        "\n",
        "\n",
        "# Function to construct the path for saving and retrieving model weights\n",
        "def get_weights_file_path(config, epoch: str):\n",
        "    model_folder = config['model_folder'] # Extracting model folder from the config\n",
        "    model_basename = config['model_basename'] # Extracting the base name for model files\n",
        "    model_filename = f\"{model_basename}{epoch}.pt\" # Building filename\n",
        "    return str(Path('.')/ model_folder/ model_filename)\n",
        "def train_model(config):\n",
        "    # Setting up device to run on GPU to train faster\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device {device}\")\n",
        "\n",
        "    # Creating model directory to store weights\n",
        "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Retrieving dataloaders and tokenizers for source and target languages using the 'get_ds' function\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "\n",
        "    # Initializing model on the GPU using the 'get_model' function\n",
        "    model = get_model(config,tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "    # Tensorboard\n",
        "    writer = SummaryWriter(config['experiment_name'])\n",
        "\n",
        "    # Setting up the Adam optimizer with the specified learning rate from the '\n",
        "    # config' dictionary plus an epsilon value\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps = 1e-9)\n",
        "\n",
        "    # Initializing epoch and global step variables\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "\n",
        "    # Checking if there is a pre-trained model to load\n",
        "    # If true, loads it\n",
        "    if config['preload']:\n",
        "        model_filename = get_weights_file_path(config, config['preload'])\n",
        "        print(f'Preloading model {model_filename}')\n",
        "        state = torch.load(model_filename) # Loading model\n",
        "\n",
        "        # Sets epoch to the saved in the state plus one, to resume from where it stopped\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        # Loading the optimizer state from the saved model\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        # Loading the global step state from the saved model\n",
        "        global_step = state['global_step']\n",
        "\n",
        "    # Initializing CrossEntropyLoss function for training\n",
        "    # We ignore padding tokens when computing loss, as they are not relevant for the learning process\n",
        "    # We also apply label_smoothing to prevent overfitting\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing = 0.1).to(device)\n",
        "\n",
        "    # Initializing training loop\n",
        "\n",
        "    # Iterating over each epoch from the 'initial_epoch' variable up to\n",
        "    # the number of epochs informed in the config\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "\n",
        "        # Initializing an iterator over the training dataloader\n",
        "        # We also use tqdm to display a progress bar\n",
        "        batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch:02d}')\n",
        "\n",
        "        # For each batch...\n",
        "        for batch in batch_iterator:\n",
        "            model.train() # Train the model\n",
        "\n",
        "            # Loading input data and masks onto the GPU\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            decoder_input = batch['decoder_input'].to(device)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)\n",
        "            decoder_mask = batch['decoder_mask'].to(device)\n",
        "\n",
        "            # Running tensors through the Transformer\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "            proj_output = model.project(decoder_output)\n",
        "\n",
        "            # Loading the target labels onto the GPU\n",
        "            label = batch['label'].to(device)\n",
        "\n",
        "            # Computing loss between model's output and true labels\n",
        "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "\n",
        "            # Updating progress bar\n",
        "            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "            writer.add_scalar('train loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            # Performing backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            # Updating parameters based on the gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            # Clearing the gradients to prepare for the next batch\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            global_step += 1 # Updating global step count\n",
        "\n",
        "        # We run the 'run_validation' function at the end of each epoch\n",
        "        # to evaluate model performance\n",
        "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
        "\n",
        "        # Saving model\n",
        "        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
        "        # Writting current model state to the 'model_filename'\n",
        "        torch.save({\n",
        "            'epoch': epoch, # Current epoch\n",
        "            'model_state_dict': model.state_dict(),# Current model state\n",
        "            'optimizer_state_dict': optimizer.state_dict(), # Current optimizer state\n",
        "            'global_step': global_step # Current global step\n",
        "        }, model_filename)"
      ],
      "metadata": {
        "id": "dAqQenL6pU6j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    warnings.filterwarnings('ignore') # Filtering warnings\n",
        "    config = get_config() # Retrieving config settings\n",
        "    train_model(config) # Training model with the config arguments"
      ],
      "metadata": {
        "collapsed": true,
        "id": "G0IiZ-V8CTgm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}