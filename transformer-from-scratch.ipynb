{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -U datasets huggingface_hub fsspec","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import get_dataset_config_names\nget_dataset_config_names('opus_books')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T14:07:02.806499Z","iopub.execute_input":"2025-07-12T14:07:02.806926Z","iopub.status.idle":"2025-07-12T14:07:06.596517Z","shell.execute_reply.started":"2025-07-12T14:07:02.806901Z","shell.execute_reply":"2025-07-12T14:07:06.595924Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e67e08acb8746f2a5824020ef35a375"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"['ca-de',\n 'ca-en',\n 'ca-hu',\n 'ca-nl',\n 'de-en',\n 'de-eo',\n 'de-es',\n 'de-fr',\n 'de-hu',\n 'de-it',\n 'de-nl',\n 'de-pt',\n 'de-ru',\n 'el-en',\n 'el-es',\n 'el-fr',\n 'el-hu',\n 'en-eo',\n 'en-es',\n 'en-fi',\n 'en-fr',\n 'en-hu',\n 'en-it',\n 'en-nl',\n 'en-no',\n 'en-pl',\n 'en-pt',\n 'en-ru',\n 'en-sv',\n 'eo-es',\n 'eo-fr',\n 'eo-hu',\n 'eo-it',\n 'eo-pt',\n 'es-fi',\n 'es-fr',\n 'es-hu',\n 'es-it',\n 'es-nl',\n 'es-no',\n 'es-pt',\n 'es-ru',\n 'fi-fr',\n 'fi-hu',\n 'fi-no',\n 'fi-pl',\n 'fr-hu',\n 'fr-it',\n 'fr-nl',\n 'fr-no',\n 'fr-pl',\n 'fr-pt',\n 'fr-ru',\n 'fr-sv',\n 'hu-it',\n 'hu-nl',\n 'hu-no',\n 'hu-pl',\n 'hu-pt',\n 'hu-ru',\n 'it-nl',\n 'it-pt',\n 'it-ru',\n 'it-sv']"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.utils.tensorboard import SummaryWriter\nimport math\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport warnings\nclass InputEmbeddings(nn.Module):\n  def __init__(self, embedding_dim, vocab_size):   # embedding_dim is d_model\n    super().__init__()\n    self.embedding_dim = embedding_dim\n    self.vocab_size = vocab_size\n    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n\n  def forward(self, x):\n    return self.embedding(x) * math.sqrt(self.embedding_dim) # scale embeddings to match positional encoding scale\nclass PositionalEncoding(nn.Module):\n  def __init__(self, embedding_dim, sequence_len, dropout):\n    super().__init__()\n    self.embedding_dim = embedding_dim\n    self.sequence_len = sequence_len\n    self.dropout = nn.Dropout(dropout)\n\n    PE = torch.zeros(sequence_len, embedding_dim)\n    position = torch.arange(0, sequence_len, dtype = torch.float).unsqueeze(1)\n\n    denominator_term = torch.exp(torch.arange(0, embedding_dim, step = 2).float() * (-math.log(10000.0) / embedding_dim))\n\n    PE[:, 0::2] = torch.sin(position * denominator_term)\n    PE[:, 1::2] = torch.cos(position * denominator_term)\n    PE = PE.unsqueeze(0)\n\n    self.register_buffer('PE', PE)\n\n  def forward(self, x):\n    x = x + (self.PE[:, :x.shape[1], :]).requires_grad_(False)\n    return self.dropout(x)\n\nclass MultiHeadAttentionBlock(nn.Module):\n  def __init__(self, embedding_dim, h, dropout):\n    super().__init__()\n    self.embedding_dim = embedding_dim\n    self.h = h\n\n    assert embedding_dim % h == 0, \"embedding_dim is not divisible by h\"\n\n    self.d_k = embedding_dim // h\n    self.w_q = nn.Linear(embedding_dim, embedding_dim)\n    self.w_k = nn.Linear(embedding_dim, embedding_dim)\n    self.w_v = nn.Linear(embedding_dim, embedding_dim)\n    self.w_o = nn.Linear(embedding_dim, embedding_dim)\n\n    self.dropout = nn.Dropout(dropout)\n\n  @staticmethod\n  def attention(query, key, value, mask, dropout):\n    d_k = query.shape[-1]\n    attention_score = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n      attention_score.masked_fill_(mask == 0, -1e9)\n    attention_score = attention_score.softmax(dim = -1)\n    if dropout is not None:\n      attention_score = dropout(attention_score)\n    return (attention_score @ value), attention_score\n\n  def forward(self, q, k, v, mask):\n    query = self.w_q(q)\n    key = self.w_k(k)\n    value = self.w_v(v)\n\n    query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n    key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n    value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n\n    x, self.attention_score = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n    x = x.transpose(2,1).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n    return self.w_o(x)\n\nclass LayerNormalization(nn.Module):\n  def __init__(self, eps = 10**-6):\n    super().__init__()\n    self.eps = eps\n\n    self.gamma = nn.Parameter(torch.ones(1))\n    self.beta = nn.Parameter(torch.zeros(1))\n\n  def forward(self, x):\n    mean = x.mean(dim=-1, keepdim=True)\n    std = x.std(dim=-1, keepdim=True)\n    return (self.gamma * ((x - mean) / (std + self.eps))) + self.beta\n\nclass FeedForwardBlock(nn.Module):\n  def __init__(self, d_model, d_ff, dropout):\n    super().__init__()\n    self.linear_1 = nn.Linear(d_model, d_ff)\n    self.dropout = nn.Dropout(dropout)\n    self.linear_2 = nn.Linear(d_ff, d_model)\n\n  def forward(self, x):\n    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n\nclass ResidualConnection(nn.Module):\n  def __init__(self, dropout):\n    super().__init__()\n    self.dropout = nn.Dropout(dropout)\n    self.norm = LayerNormalization()\n\n  def forward(self, x, sublayer):\n    return x + self.dropout(sublayer(self.norm(x)))\n\nclass EncoderBlock(nn.Module):\n  def __init__(self, self_attention_block, feed_forward_block, dropout):\n    super().__init__()\n    self.self_attention_block = self_attention_block\n    self.feed_forward_block = feed_forward_block\n\n    self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n\n  def forward(self, x, src_mask):\n      x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n      x = self.residual_connections[1](x, self.feed_forward_block)\n      return x\n\nclass Encoder(nn.Module):\n  def __init__(self, layers):\n    super().__init__()\n    self.layers = layers\n    self.norm = LayerNormalization()\n\n  def forward(self, x, mask):\n    for layer in self.layers:\n      x = layer(x, mask)\n    return self.norm(x)\n\nclass DecoderBlock(nn.Module):\n  def __init__(self, self_attention_block, cross_attention_block, feed_forward_block, dropout):\n    super().__init__()\n    self.self_attention_block = self_attention_block\n    self.cross_attention_block = cross_attention_block\n    self.feed_forward_block = feed_forward_block\n\n    self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n\n  def forward(self, x, encoder_output, src_mask, target_mask):\n    x = self.residual_connections[0](x, lambda x:self.self_attention_block(x,x,x,target_mask))\n    x = self.residual_connections[1](x, lambda x:self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n    x = self.residual_connections[2](x, self.feed_forward_block)\n    return x\n\nclass Decoder(nn.Module):\n  def __init__(self, layers):\n    super().__init__()\n    self.layers = layers\n\n    self.norm = LayerNormalization()\n\n  def forward(self, x, encoder_output, src_mask, target_mask):\n    for layer in self.layers:\n      x = layer(x, encoder_output, src_mask, target_mask)\n    return self.norm(x)\n\nclass ProjectionLayer(nn.Module):\n  def __init__(self, embedding_dim, vocab_size):\n    super().__init__()\n\n    self.projection = nn.Linear(embedding_dim, vocab_size)\n\n  def forward(self,x):\n    return torch.log_softmax(self.projection(x), dim=-1)\n\nclass Transformer(nn.Module):\n  def __init__(self, encoder, decoder, src_embedding, target_embedding, src_positional_encoding, target_positional_encoding, projection_layer):\n    super().__init__()\n    self.encoder = encoder\n    self.decoder = decoder\n    self.src_embedding = src_embedding\n    self.target_embedding = target_embedding\n    self.src_positional_encoding = src_positional_encoding\n    self.target_positional_encoding = target_positional_encoding\n    self.projection_layer = projection_layer\n\n  def encode(self, src, src_mask):\n    src = self.src_embedding(src)\n    src = self.src_positional_encoding(src)\n    return self.encoder(src, src_mask)\n\n  def decode(self, encoder_output, src_mask, target, target_mask):\n    target = self.target_embedding(target)\n    target = self.target_positional_encoding(target)\n    return self.decoder(target, encoder_output, src_mask, target_mask)\n\n  def project(self, x):\n    return self.projection_layer(x)\ndef build_transformer(src_vocab_size, target_vocab_size, src_seq_len, target_seq_len, embedding_dim = 512, N = 6, h = 8, dropout = 0.1, d_ff = 2048):\n  src_embedding = InputEmbeddings(embedding_dim, src_vocab_size)\n  target_embedding = InputEmbeddings(embedding_dim, target_vocab_size)\n  src_positional_encoding = PositionalEncoding(embedding_dim, src_seq_len, dropout)\n  target_positional_encoding = PositionalEncoding(embedding_dim, target_seq_len, dropout)\n\n  encoder_blocks = []\n  for _ in range(N):\n    encoder_self_attention_block = MultiHeadAttentionBlock(embedding_dim, h, dropout)\n    feed_forward_block = FeedForwardBlock(embedding_dim, d_ff, dropout)\n    encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n    encoder_blocks.append(encoder_block)\n\n  decoder_blocks = []\n  for _ in range(N):\n    decoder_self_attention_block = MultiHeadAttentionBlock(embedding_dim, h, dropout)\n    decoder_cross_attention_block = MultiHeadAttentionBlock(embedding_dim, h, dropout)\n    feed_forward_block = FeedForwardBlock(embedding_dim, d_ff, dropout)\n    decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n    decoder_blocks.append(decoder_block)\n\n  encoder = Encoder(nn.ModuleList(encoder_blocks))\n  decoder = Decoder(nn.ModuleList(decoder_blocks))\n\n  projection_layer = ProjectionLayer(embedding_dim, target_vocab_size)\n  transformer = Transformer(encoder, decoder, src_embedding, target_embedding, src_positional_encoding, target_positional_encoding, projection_layer)\n\n  for p in transformer.parameters():\n    if p.dim() > 1:\n      nn.init.xavier_uniform_(p)\n\n  return transformer\n\ndef get_all_sentences(ds, lang):\n    for pair in ds:\n        yield pair['translation'][lang]\n\nclass BilingualDataset(Dataset):\n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n        super().__init__()\n\n        self.seq_len = seq_len\n        self.ds = ds\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, index) :\n        src_target_pair = self.ds[index]\n        src_text = src_target_pair['translation'][self.src_lang]\n        tgt_text = src_target_pair['translation'][self.tgt_lang]\n\n        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n\n        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 \n\n        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n            raise ValueError('Sentence is too long')\n\n        encoder_input = torch.cat(\n            [\n            self.sos_token, \n            torch.tensor(enc_input_tokens, dtype = torch.int64), \n            self.eos_token, \n            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64) \n            ]\n        )\n        decoder_input = torch.cat(\n            [\n                self.sos_token, \n                torch.tensor(dec_input_tokens, dtype = torch.int64),\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) \n            ]\n\n        )\n\n        label = torch.cat(\n            [\n                torch.tensor(dec_input_tokens, dtype = torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) \n\n            ]\n        )\n\n        assert encoder_input.size(0) == self.seq_len\n        assert decoder_input.size(0) == self.seq_len\n        assert label.size(0) == self.seq_len\n\n        return {\n            'encoder_input': encoder_input,\n            'decoder_input': decoder_input,\n            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n            'label': label,\n            'src_text': src_text,\n            'tgt_text': tgt_text\n        }\n\ndef get_ds(config):\n    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split = 'train')\n\n    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n\n    train_ds_size = int(0.9 * len(ds_raw)) \n    val_ds_size = len(ds_raw) - train_ds_size \n    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size]) \n    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n\n    max_len_src = 0\n    max_len_tgt = 0\n    for pair in ds_raw:\n        src_ids = tokenizer_src.encode(pair['translation'][config['lang_src']]).ids\n        tgt_ids = tokenizer_src.encode(pair['translation'][config['lang_tgt']]).ids\n        max_len_src = max(max_len_src, len(src_ids))\n        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n\n    print(f'Max length of source sentence: {max_len_src}')\n    print(f'Max length of target sentence: {max_len_tgt}')\n\n    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True)\n    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n\n    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt \n\ndef casual_mask(size):\n  \n        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n        return mask == 0\n\ndef build_tokenizer(config, ds, lang):\n\n    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n    if not Path.exists(tokenizer_path):\n        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]'))\n        tokenizer.pre_tokenizer = Whitespace()\n\n        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\",\n                                                     \"[SOS]\", \"[EOS]\"], min_frequency = 2)\n\n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer = trainer)\n        tokenizer.save(str(tokenizer_path))\n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n    return tokenizer\n\ndef greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n  \n    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n\n    encoder_output = model.encode(source, source_mask)\n    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n\n    while True:\n        if decoder_input.size(1) == max_len:\n            break\n\n        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n\n        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n\n        prob = model.project(out[:, -1])\n\n        _, next_word = torch.max(prob, dim=1)\n        decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n\n        if next_word == eos_idx:\n            break\n\n    return decoder_input.squeeze(0)\n\ndef run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n    model.eval()\n    count = 0 \n\n    console_width = 80 \n    with torch.no_grad(): \n        for batch in validation_ds:\n            count += 1\n            encoder_input = batch['encoder_input'].to(device)\n            encoder_mask = batch['encoder_mask'].to(device)\n\n            assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n\n            source_text = batch['src_text'][0]\n            target_text = batch['tgt_text'][0] \n            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) \n\n            print_msg('-'*console_width)\n            print_msg(f'SOURCE: {source_text}')\n            print_msg(f'TARGET: {target_text}')\n            print_msg(f'PREDICTED: {model_out_text}')\n\n            if count == num_examples:\n                break\n\ndef get_model(config, vocab_src_len, vocab_tgt_len):\n\n    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n    return model\n\ndef get_config():\n    return{\n        'batch_size': 8,\n        'num_epochs': 2,\n        'lr': 10**-4,\n        'seq_len': 350,\n        'd_model': 512, \n        'lang_src': 'en',\n        'lang_tgt': 'it',\n        'model_folder': 'weights',\n        'model_basename': 'tmodel_',\n        'preload': None,\n        'tokenizer_file': 'tokenizer_{0}.json',\n        'experiment_name': 'runs/tmodel'\n    }\n\ndef get_weights_file_path(config, epoch: str):\n    model_folder = config['model_folder'] \n    model_basename = config['model_basename'] \n    model_filename = f\"{model_basename}{epoch}.pt\" \n    return str(Path('.')/ model_folder/ model_filename)\n\ndef train_model(config):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device {device}\")\n\n    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n\n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n\n    model = get_model(config,tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n\n    writer = SummaryWriter(config['experiment_name'])\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps = 1e-9)\n\n    initial_epoch = 0\n    global_step = 0\n\n    if config['preload']:\n        model_filename = get_weights_file_path(config, config['preload'])\n        print(f'Preloading model {model_filename}')\n        state = torch.load(model_filename) \n\n        initial_epoch = state['epoch'] + 1\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        global_step = state['global_step']\n\n    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing = 0.1).to(device)\n\n\n    for epoch in range(initial_epoch, config['num_epochs']):\n\n        batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch:02d}')\n\n        for batch in batch_iterator:\n            model.train() \n\n            encoder_input = batch['encoder_input'].to(device)\n            decoder_input = batch['decoder_input'].to(device)\n            encoder_mask = batch['encoder_mask'].to(device)\n            decoder_mask = batch['decoder_mask'].to(device)\n\n            encoder_output = model.encode(encoder_input, encoder_mask)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n            proj_output = model.project(decoder_output)\n\n            label = batch['label'].to(device)\n\n            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n\n            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n\n            writer.add_scalar('train loss', loss.item(), global_step)\n            writer.flush()\n\n            loss.backward()\n\n            optimizer.step()\n\n            optimizer.zero_grad()\n\n            global_step += 1 \n\n        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n\n        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n        torch.save({\n            'epoch': epoch, \n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(), \n            'global_step': global_step \n        }, model_filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T14:07:28.671418Z","iopub.execute_input":"2025-07-12T14:07:28.671715Z","iopub.status.idle":"2025-07-12T14:07:28.721151Z","shell.execute_reply.started":"2025-07-12T14:07:28.671693Z","shell.execute_reply":"2025-07-12T14:07:28.720450Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"if __name__ == '__main__':\n    warnings.filterwarnings('ignore') \n    config = get_config() \n    train_model(config) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T14:07:58.146586Z","iopub.execute_input":"2025-07-12T14:07:58.146864Z","iopub.status.idle":"2025-07-12T14:39:12.525886Z","shell.execute_reply.started":"2025-07-12T14:07:58.146841Z","shell.execute_reply":"2025-07-12T14:39:12.525310Z"}},"outputs":[{"name":"stdout","text":"Using device cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/5.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d514f14951f64332821cd8760d09048b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a64e25ecca024023bd89f1663ec016e6"}},"metadata":{}},{"name":"stdout","text":"Max length of source sentence: 309\nMax length of target sentence: 274\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch 00: 100%|██████████| 3638/3638 [15:28<00:00,  3.92it/s, loss=4.880]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: I could not unlove him, because I felt sure he would soon marry this very lady--because I read daily in her a proud security in his intentions respecting her--because I witnessed hourly in him a style of courtship which, if careless and choosing rather to be sought than to seek, was yet, in its very carelessness, captivating, and in its very pride, irresistible.\nTARGET: Non potevo cessar di amarlo perché capivo che avrebbe sposato presto quella ragazza; perché leggevo nel contegno della signorina Ingram l'altera sicurezza del trionfo, perché infine a ogni istante scoprivo nel signor Rochester una specie di cortesia, che nonostante fosse imposta, più che data, era irresistibile nella sua noncuranza e nel suo orgoglio.\nPREDICTED: \" Ma non mi , ma non mi , e non mi , e , e , e , e , e , e , e , e , e , e , e , e , e , e , e , e , e , e a un ' altra .\n--------------------------------------------------------------------------------\nSOURCE: \"I would consent to be at your mercy, Jane.\"\nTARGET: — Acconsentirei a essere nelle vostre mani, Jane.\nPREDICTED: — Sì , signore , signore , — mi .\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch 01: 100%|██████████| 3638/3638 [15:28<00:00,  3.92it/s, loss=4.297]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: \"Pooh! you can't be silly enough to wish to leave such a splendid place?\"\nTARGET: — Non siete, spero, tanto stupida da desiderare di andarvene.\nPREDICTED: — Non è vero , non è stata più più più di nuovo ?\n--------------------------------------------------------------------------------\nSOURCE: I was obliged to...'\nTARGET: Ho dovuto....\nPREDICTED: Io mi .\n","output_type":"stream"}],"execution_count":5}]}